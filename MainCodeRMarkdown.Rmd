---
title: "Family Income in Relation to Student Progress"
author: "Kashish Rai, Karin Funaki, Chenzi Xiu, Eric Trang"
date: "December 11, 2024"
output:
  html_document: default
  pdf_document: default
---
#### Question: Does family income affect the variables of Extracurrciular Activities, Access to Resources, Hours Studied, Parental Involvement, Tutoring Sessions, School Type, Parental Educational Level? Does the effect of family income on these variables affect the average educational progress of a student? 

##### R-Install 

**Installing all needed packages and running all the necessary libraries**

```{r install}
if(!require(knitr)){
    install.packages("knitr", repos = "https://cran.r-project.org")
    library(knitr)
}

if(!require(tidyverse)){
  install.packages("tidyverse")
  library(tidyverse)
  library(dplyr)
}

if(!require(ggforce)){
 install.packages("ggforce")
  library(ggforce)
}  

if(!require(ggplot2)){
 install.packages("ggplot2")
  library(ggplot2)
}  

if(!require(naniar)){
 install.packages("naniar")
  library(naniar)
}  

if(!require(dplyr)){
 install.packages("dplyr")
  library(dplyr)
}  

if(!require(car)) install.packages("car")
library(car)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##### Initial Data Clean + Set-Up

```{r}
# read initial data
studentperformance.initial <- read.csv("StudentPerformanceFactors.csv")
head(studentperformance.initial, 3)
```

```{r}
# clean data
# check if there are any NA values 
sum(is.na(studentperformance.initial))
sum(complete.cases(studentperformance.initial))

#still -- saw some values that were blank, so defined them
common_na_strings <- c("", " ", "NA", "N/A", "-", "null", "NULL")

#applied this string to the entire data set
studentperformance.initial %>%
  miss_scan_count(search = common_na_strings)

# proved that there are blanks -- replace blanks across all columns
studentperformance.na <- studentperformance.initial %>%
  mutate(across(everything(), ~ ifelse(trimws(.) == "", NA, .)))

# display all NA blanks in table
table(is.na(studentperformance.na))

# make sure the NA replacement worked by looking at new data frame
head(studentperformance.na, 3)

#omit all NA values from the data 
cleaned_data <- na.omit(studentperformance.na)
if (nrow(cleaned_data) == 0) {
  stop("All rows contain missing values. Cannot proceed with model.")
}

#neatly organize and display cleaned data
str(cleaned_data)
head(cleaned_data, 3)
```

```{r}
# change the class type of all categorical variables to factor levels so they could be seen as categories when analyzing using r. continuous variables being the integer type is good, so it was kept the same
cleaned_data[] <- lapply(cleaned_data, function(x) {
  if (is.character(x)) 
    as.factor(x) 
  else 
    x
})

sapply(cleaned_data, class)

studentperformance <- cleaned_data

head(studentperformance, 3)
```

```{r}
# create new column for student progress
studentperformance$Progress <- studentperformance$Exam_Score - studentperformance$Previous_Scores
head(studentperformance, 3)
```

**Measures of Central Tendencies of Numerical Variables***

##### Averages of Numerical Variables

```{r}
#see what kind of variables Hours_Studied and Tutoring Sessions Are 
class(studentperformance$Hours_Studied)

class(studentperformance$Tutoring_Sessions)

#since these are integer variables: calculate the mean, median, mode and frequency distribution

# list of interest variables to calculate statistics for
integer.interest.variables <- c("Hours_Studied", "Tutoring_Sessions")

#loop through each integer variable and calculate the mean
for (var in integer.interest.variables) {
  mean_value <- mean(studentperformance[[var]])
  
# print the results
  cat("Mean for", var, ":", mean_value, "\n")
}

#loop through each interest variable and calculate the median
for (var in integer.interest.variables) {
  median_value <- median(studentperformance[[var]])
  
# print the results
  cat("Median for", var, ":", median_value, "\n")
}

# calculate mode
# define a function for mode calculation
mode <- function(x) {
  uniq_x <- unique(x)
  uniq_x[which.max(tabulate(match(x, uniq_x)))]  # return the most frequent value
}

#loop through each interest variable and calculate the mode
for (var in integer.interest.variables) {
  mode_value <- mode(studentperformance[[var]])
  
# print the results
  cat("Mode for", var, ":", mode_value, "\n")
}

#calculate frequency distribution
#loop through each interest variable and calculate the frequency distribution
for (var in integer.interest.variables) {
  freq_table <- table(studentperformance[[var]])

# print the frequency table
  cat("Frequency distribution for", var, ":\n")
  print(freq_table)
  cat("---------------------------------\n")
}

#create histograms
#loop through each interest variable and calculate the frequency distribution
for (var in integer.interest.variables) {
  freq_hist <- hist(studentperformance[[var]], main = var, xlab = var)

# output histograms
  freq_hist
}

```


Looking at our integer variables of interest here, it looks like an average amount of all surveyed students study for about 19.9 hours and have 1.5 tutoring sessions. This value does not change much throughout the other averages of median and mode: with study hours being around 20 hours for both and tutoring sessions being 1. This tells that most students without any differentiation by other variables study around those averages. Looking through the frequency distribution data, it looks like most students study in the data set study between 11-30 hours. It also looks like most students get between 0-2 tutoring sessions, with a smaller range beting frmo 3-5.

##### Visualization

###### Barplot 

```{r}
# barplot visualization of variables of interest: Extracurrciular_Activities, Access_to_Resources, Hours Studied, Parental_Involvement, Tutoring_Sessions, School_Type, Parental_Educational_Level 

#create the interest variables that we need to graph
interest.variables <- c("Extracurricular_Activities", "Access_to_Resources", "Hours_Studied", 
               "Parental_Involvement", "Tutoring_Sessions", "School_Type", 
               "Parental_Education_Level")

# loop through each variable and create a bar chart
for (var in interest.variables) {
  barplot <- ggplot(studentperformance, aes(x = !!sym(var), fill = Family_Income)) +
    geom_bar(position = "dodge") +
    labs(title = paste("Distribution of", var, "by Family Income"),
         x = var, 
         y = "Count") +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
# print the plot
  print(barplot)
}
```


Looking through the visualization of seven interest variables in the data set that can be indicators of educational progress in students, all differentiated by income levels of the family, there are some interesting results. It does not seem like our original ideas about higher income leading to more access to educational resources is exactly true. In fact, students that come from a background that is of higher income seem to participate in less extracurricular activities as compared to their low and medium income peers. This same trend is reflected through the other interest variables as well: There is no overwhelming amount of students with high income backgrounds that access to resources, high income students spend less time studying, have less parental involvement, have lower amounts of tutoring sessions, are enrolled at lower rates in both public and private schools, and their parental educational level is at less proportions than those compared to low and middle income families.


###### Mosaic Plot

```{r}
#mosaic chart visualization of our (5) categorical variables of interest: Extracurrciular_Activities, Access_to_Resources, Parental_Involvement, School_Type, Parental_Educational_Level 

# create a new set of variables for just categorical ones we are testing 
cat.interest.variables <-  c("Extracurricular_Activities", "Access_to_Resources",
               "Parental_Involvement", "School_Type", "Parental_Education_Level")

#install package to be able to plot a mosaic plot 
if(!require(ggmosaic)){
    install.packages("ggmosaic", repos = "https://cran.r-project.org")
    library(ggmosaic)
}

#change to as factor 
studentperformance$Family_Income <- as.factor(studentperformance$Family_Income)
studentperformance[[var]] <- as.factor(studentperformance[[var]])

# loop through each variable and create a mosaic plot
for (var in cat.interest.variables) {
  mosaic.plot <- ggplot(studentperformance) +
    geom_mosaic(aes(weight = 1, x = product(Family_Income), fill = !!sym(var)), na.rm = TRUE) +
    labs(title = paste("Mosaic Plot of", var, "by Family Income"),
         x = "Family Income",
         y = "Proportion",
         fill = var) +
    theme_minimal() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
# print the plot
  print(mosaic.plot)
}
```


Looking through just our five categorical variables of interest, we decided to go with a mosaic plot to represent the different data. Here, it looks like family income does not have a very significant effect on the proportions of the data. Really, it actually looks like the only income level that has different proportions than the others are low income families, with most differences in access to resources, parental involvement, and school type. This tells us that it is probable that low income families were the most surveyed in this study. These graphs also help us to know that there is not a significant proportional difference in the educational affecting variables between income groups!

###### Boxplot

```{r}
#boxplot chart visualization of our (2) integer variables of interest: Hours_Studied and Tutoring Sessions

#identify variables of interest
integer.interest.variables 
x_variable <- "Family_Income"

# loop through each variable of interest to create boxplots
for (var in integer.interest.variables) {
 
  
  
   boxplot <- ggplot(studentperformance, aes(x = .data[[x_variable]], y = .data[[var]])) +
    geom_boxplot(fill = "skyblue", color = "darkblue", outlier.color = "red") +
    theme_minimal() +
    labs(title = paste(var, "vs", x_variable),
         x = x_variable,
         y = var) +
    theme(plot.title = element_text(hjust = 0.5, size = 14),
          axis.title = element_text(size = 12))
  
# print the plot
  print(boxplot)
}
```


Here, we used boxplots for our two integer variables of interest, because we wanted to have a visual representation of the averages differentiated across family income. In the case of the hours studied, it seems like there is no difference between the data across family income at all from the 25th to the 75th percentiles. It does seem like, however, that low and medium income families do seem to have many more outliers as compared to high family income. This tells us that the spread for those two income types is wider than high income spread. Looking through the tutoring sessions, the 25th to 75th percentile spread seems to be the same, but once again, the outliers for medium family income tells us that the spread is different than low and high family income.

##### Chi-Square Test

###### Assumptions for Chi-Square Test

```{r}
#check for assumptions for chi-square

# loop through each variable to check assumptions
for (var in cat.interest.variables) {
  # Create a contingency table
  contingency_table <- table(studentperformance[[var]], studentperformance$Family_Income)
  
  # Check independence assumption (cannot directly test but mention it's assumed)
  cat("Checking assumptions for", var, "vs Family_Income:\n")
  cat("Independence of observations: Assumed based on data collection.\n")
  
  # Calculate expected frequencies
  expected_freq <- chisq.test(contingency_table)$expected
  
  # Check if all expected frequencies are >= 5
  if (all(expected_freq >= 5)) {
    cat("All expected frequencies are >= 5. The Chi-Square test assumption is met.\n")
  } else {
    cat("Warning: Some expected frequencies are < 5. The Chi-Square test assumption is violated.\n")
    cat("Expected frequencies:\n")
    print(expected_freq)
  }
  
  cat("\n-------------------------------------------\n")
}
```


All assumptions for the Chi-Square Test have been met to run for categorical interest variables.

###### Chi-Square Test Code 

```{r}
#chi-square analysis of our categorical variables of interest: Extracurricular_Activities, Access_to_Resources, Parental_Involvement, School_Type, Parental_Educational_Level 
  
# loop through each variable and run the Chi-Square test
for (var in cat.interest.variables) {
  contingency_table <- table(studentperformance[[var]], studentperformance$Family_Income)
  chi_test <- chisq.test(contingency_table)
  
# display the results
  cat("Chi-Square Test for", var, "vs Family_Income:\n")
  print(chi_test)
  cat("\n-------------------------------------------\n")
}
```


For all categorical interest variables: It looks to be that the p-value of this chi-square test for the categorical variables is larger than 0.05. This means that there is not much significant data to signify a correlation between the variables and family income. This conclusion has actually also been shown by the visualizations that we conducted previously. It does not seem like there is much difference between family income in the surveyed group and the variables that affect their educational variables. This means that if there was some difference between progress through the family income, it would not be because of changes in these specific categorical variables.

**Refined Question: Is there a specific factor (out of the factors that we chose previously) that is affected by family income and in turn, affecting educational progress?**

##### ANOVA Test

###### First, we will perform an ANOVA Test for family income on progress since family income is the main variable we are interested in. 
###### Assumptions of ANOVA Test

```{r}
# The conditions for ANOVA are independence, normality, and homoskedasticity. We will assume that there is independence between and within family income groups, because it's not likely that one family's income is impacted by another family's income.

# mean, standard deviation, number of observations, and progress for each family income group
studentperformance %>%
  group_by(Family_Income) %>%
  summarize(
    Mean = mean(Progress),
    SD = sd(Progress),
    n = n(),
    SE = sd(Progress) / sqrt(n())
  )

# histogram
studentperformance %>%
  ggplot(aes(x = Progress)) +
  geom_histogram(aes(fill = Family_Income)) +
  facet_grid(cols = vars(Family_Income))

# boxplot
studentperformance %>%
  ggplot(aes(x = Family_Income, y = Progress)) +
  geom_boxplot(aes(fill = Family_Income)) +
  labs(
    x = "Family Income",
    y = "Progress"
  )
```


Although the boxplots show that there is one outlier for the medium family income group, we will not remove this since it's not as extreme to the point that it changes the variability of data drastically and is unlikely that it's caused by a measurement error. Overall, data from all family income groups seem to be nearly normal and the variability across those groups seem to be not significantly different, meeting the conditions for an ANOVA test.

###### ANOVA Test Code + Post-Hoc Test

```{r}
# Fit the model
family_income_fit <- lm(Progress ~ Family_Income, data = studentperformance)

# Perform ANOVA
family_income_anova <- anova(family_income_fit)
cat("\nANOVA Results for Family_Income:\n")
print(family_income_anova)

# Check the p-value from ANOVA
p_value <- family_income_anova$`Pr(>F)`[1]
if (p_value < 0.05) {
  cat("\nSignificant effect detected (p < 0.05). Proceeding with post hoc tests...\n")
  
  # Residual diagnostics: Normality test
  ks_test <- ks.test(residuals(family_income_fit), "pnorm", 
                     mean = mean(residuals(family_income_fit)), 
                     sd = sd(residuals(family_income_fit)))
  cat("\nKolmogorov-Smirnov test for normality of residuals:\n")
  print(ks_test)
  
  # Homogeneity of variances: Levene's test
  levene_test <- car::leveneTest(Progress ~ Family_Income, data = studentperformance)
  cat("\nLevene's test for homogeneity of variances:\n")
  print(levene_test)
  
  # Post hoc tests
  if (levene_test$`Pr(>F)`[1] > 0.05) {
    # Variances are equal: Use Tukey's HSD
    posthoc_result <- TukeyHSD(aov(family_income_fit))
    cat("\nTukeyHSD Post-Hoc Test Results:\n")
    print(posthoc_result)
  } else {
    # Variances are not equal: Use Pairwise t-tests with p-value adjustment
    posthoc_result <- pairwise.t.test(
      studentperformance$Progress, 
      studentperformance$Family_Income, 
      p.adjust.method = "holm"
    )
    cat("\nPairwise t-test with Holm correction (unequal variances):\n")
    print(posthoc_result)
  }
} else {
  cat("\nNo significant effect detected (p >= 0.05). Post hoc tests are not necessary.\n")
}

```


There is some significance in the difference of exam score progress between family income groups.

###### Next, we will perform ANOVA tests for all categorical variables we tested above. We will assume independence for all variables since it's most likely that a student's home environment is not influenced by other students' home environments.
###### Assumptions of ANOVA Test

```{r}
for (var in cat.interest.variables){
  studentperformance %>%
    group_by(!!sym(var)) %>% # sym() makes a string into a symbol, !! unquotes
    summarize(
      Mean = mean(Progress),
      SD = sd(Progress),
      n = n(),
      SE = sd(Progress) / sqrt(n())
    ) %>%
    print()
  
  hist <- studentperformance %>%
    ggplot(aes(x = Progress)) +
    geom_histogram(aes(fill = !!sym(var))) +
    facet_grid(cols = vars(!!sym(var)))
  
  print(hist)
  
  
  bp <- studentperformance %>%
    ggplot(aes(x = var, y = Progress)) +
    geom_boxplot(aes(fill = !!sym(var))) +
    labs(
      x = var,
      y = "Progress"
    )
    
  print(bp)
}
```


All variables have outliers shown on the boxplots, but we will keep them for analysis since they are not too extreme to change their variabilities drastically.

###### ANOVA Test Code + Post-Hoc Test 

```{r}
library(car) # For Levene's Test

anova_valid_cat_var <- c("Extracurricular_Activities", "Access_to_Resources",
                         "Parental_Involvement", "School_Type")

dependent_var <- "Progress"

# Initialize vectors to store results
p_values <- numeric(length(anova_valid_cat_var))
names(p_values) <- anova_valid_cat_var

# Store significant variables for post hoc testing
significant_vars <- list()

# Perform ANOVA for each variable
for (i in seq_along(anova_valid_cat_var)) {
  var <- anova_valid_cat_var[i]
  formula <- as.formula(paste(dependent_var, "~", var))
  var_fit <- lm(formula, data = studentperformance)
  var_anova <- anova(var_fit)
  
  # Print ANOVA results
  cat("\nANOVA Results for:", var, "\n")
  print(var_anova)
  
  # Extract p-value
  p_values[i] <- var_anova$`Pr(>F)`[1]
  
  # Store the variable if p-value < 0.05
  if (p_values[i] < 0.05) {
    significant_vars[[var]] <- var_fit
  }
}

# Output all p-values
cat("\nP-values from ANOVA:\n")
print(p_values)

# Perform post hoc tests for significant variables
if (length(significant_vars) > 0) {
  for (var in names(significant_vars)) {
    cat("\nPerforming post hoc tests for:", var, "\n")
    var_fit <- significant_vars[[var]]
    formula <- as.formula(paste(dependent_var, "~", var))
    
    # Check residual normality
    ks_test <- ks.test(residuals(var_fit), "pnorm", 
                       mean = mean(residuals(var_fit)), 
                       sd = sd(residuals(var_fit)))
    cat("Kolmogorov-Smirnov test for normality:\n")
    print(ks_test)
    
    # Check homogeneity of variances
    levene_test <- car::leveneTest(formula, data = studentperformance)
    cat("Levene's test for homogeneity of variances:\n")
    print(levene_test)
    
    if (levene_test$`Pr(>F)`[1] > 0.05) {
      # Variances are equal: Use Tukey's HSD
      posthoc_result <- TukeyHSD(aov(var_fit))
      cat("\nTukeyHSD Post-Hoc Test:\n")
      print(posthoc_result)
    } else {
      # Variances are not equal: Use Pairwise t-tests with corrections
      posthoc_result <- pairwise.t.test(
        studentperformance[[dependent_var]], 
        studentperformance[[var]], 
        p.adjust.method = "holm"
      )
      cat("\nPairwise t-test with Holm correction:\n")
      print(posthoc_result)
    }
  }
} else {
  cat("\nNo variables with significant ANOVA results (p < 0.05).\n")
}

```


Only Parental_Involvement showed that there was a significant difference in progress between the groups.

##### Simple Linear Regression

###### Simple Linear Regression Code + QQPlot and Residual Graphs

```{r}

model_hours <- lm(Progress ~ Hours_Studied, data = studentperformance)
summary(model_hours)

# Simple Linear Regression Conditions Check for Final Model 
# 1. Residuals vs. Fitted Values (Linearity and Homoscedasticity)
plot(model_hours, which = 1)

# 2. Normal Q-Q Plot (Normality of Residuals)
plot(model_hours, which = 2)

# 3. Scale-Location Plot (Homoscedasticity)
plot(model_hours, which = 3)

# 4. Residuals vs Leverage Plot (Influential Points)
plot(model_hours, which = 5)

# scatter plots
for (var in integer.interest.variables){
  scatter <- ggplot(data = studentperformance) +
    geom_point(aes(x = !!sym(var), y = Progress))
  print(scatter)
}

ggplot(data = studentperformance) +
  geom_point(aes(x = Hours_Studied, y = Progress)) +
  geom_line(
    data = fortify(model_hours),
    aes(x = Hours_Studied, y = .fitted),
    color = "red"
  )
```


Looking at the scatter plots, there does not seem to be a linear trend between the number of tutoring sessions and progress in exam scores. However, there seems to be a slight linear trend between hours studied and progress, so we performed a simple linear regression for only this variable. The significance was high, and the best fit line is represented as a red line on the last scatter plot.

##### Final Conclusions


Looking through all of the data and correlation, it looks like there is a relationship between the educational progress of a student based on their family income. The seven variables that we decided to test were Extracurricular Activities, Access to Resources, Hours Studied, Parental Involvement, Tutoring Sessions, School Type, and Parental Educational Level. The reason was because we thought that there was a correlation between those variables to family income, and in the end, to educational progress. 


However, the only two of these variables that showed a correlation with family income being differentiated was Parental Involvement and Hours Studied. So, we came to the conclusion that these were the two variables that could affect the educational progress of the student through the lens of family income. We also concluded that because there was a relationship between family income and progress of a student, there could be some other variables in the data set that we did not test that could be affected by family income, which in turn could affect the progress of the student.
